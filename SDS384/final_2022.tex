
\documentclass[12pt]{article}
\usepackage{natbib,amssymb,amsmath,amsthm,epsfig,color,verbatim}
\usepackage{hyperref}
\setlength{\topmargin}{-.7in}
\setlength{\oddsidemargin}{0.3in}
\setlength{\textwidth}{6.15in}
\setlength{\textheight}{9.2in}
\renewcommand{\baselinestretch}{1.2}

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\bm}[1]{\boldsymbol{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\cov}{\text{cov}}
\newcommand{\bi}{\begin{enumerate}}
	\newcommand{\ib}{\end{enumerate}}
\newcommand{\rd}{\color{red}}
\newcommand{\bk}{\color{black}}
\newcommand{\bl}{\color{blue}}
\newcommand{\p}{\item}
% \pagestyle{empty}
\newcommand{\solution}[1] {\begin{quote}\em \color{blue} {\bf Solution}: #1 \end{quote}}
\newcommand{\grading}[1]{\begin{quote}\footnotesize \bf\em Grading: #1 \end{quote}}
% {#1} {}
%\newcommand{\grading}[1] { }

\newcommand{\Prob}{\mathbf{P}}
\newcommand{\skoo}{\vspace{.2in}}
\newcommand{\var}{\mbox{var}}
\newcommand{\gr}{\color{green}}
\newcommand{\subg}{sub-gaussian\xspace}
\newcommand{\subexp}{sub-exponential\xspace}
\newcommand{\rv}{random variable\xspace}
\newcommand{\rvs}{random variables\xspace}
\newcommand{\cd}{\stackrel{d}{\rightarrow}}
\newcommand{\cp}{\stackrel{P}{\rightarrow}}
\newcommand{\cas}{\stackrel{a.s.}{\rightarrow}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\bR}{\mathbb{R}}
\newcounter{choice}
\renewcommand\thechoice{\Alph{choice}}
\newcommand\choicelabel{\thechoice.}
\newcommand{\Exp}{\mbox{Exponential}}
\newenvironment{choices}%
{\list{\choicelabel}%
	{\usecounter{choice}\def\makelabel##1{\hss\llap{##1}}%
		\settowidth{\leftmargin}{W.\hskip\labelsep\hskip 2.5em}%
		\def\choice{%
			\item
		} % choice
		\labelwidth\leftmargin\advance\labelwidth-\labelsep
		\topsep=0pt
		\partopsep=0pt
	}%
}%
{\endlist}

\newenvironment{oneparchoices}%
{%
	\setcounter{choice}{0}%
	\def\choice{%
		\refstepcounter{choice}%
		\ifnum\value{choice}>1\relax
		\penalty -50\hskip 1em plus 1em\relax
		\fi
		\choicelabel
		\nobreak\enskip
	}% choice
	% If we're continuing the paragraph containing the question,
	% then leave a bit of space before the first choice:
	\ifvmode\else\enskip\fi
	\ignorespaces
}%
{}

\begin{document}
	
	\title{\bf   Final}
	\author{\bf SDS384}
\date{\it Spring 2022} %change for each day.
	\maketitle{}
	
	
	
	\vskip .3cm
	\vskip .3cm
	\noindent This exam has 5 short and 4 long questions. You will have to answer
	 \underline{\textbf{four short questions}}, \underline{\textbf{three long questions}}. The assigned points are noted next to each question; the total number of points is 50.  Please upload your answers in latex by 11:59 pm May 14th. Use the latex file format provided.
	\vskip .3cm
	\noindent Read each question carefully, \underline{\textbf{show your work}} and \underline{\textbf{clearly present your answers}}. If you just show the final answer without any reasoning, you will \underline{\textbf{not}} get partial credit. You will also \underline{\textbf{not}} get credit for incorrect explanation of the correct answer.
		\vskip .3cm
	\noindent You can use a fact or result in the book/lecture notes/homework. But if I gave you a problem which you have seen in the book and/or worked on already as part of your HW problems, you will still need to provide an answer \underline{\textbf{in your own words}}. 
		\vskip .3cm
			\noindent This exam is open book (Martin Wainwright's book). You can look at your class notes and HW solutions. But you cannot use \textit{any} other material.
	\vspace{1cm}
	
	
	
	\vskip 1.5cm
	\begin{center}
		{\Large \bf Good Luck!}
	\end{center}
	\vskip 1.5cm
	
	
	\noindent{\bf Name: \underline{\hskip 12.3cm}}
	\vskip 1.2cm
	\noindent{\bf UTeid: \underline{\hskip 12.3cm}}
	\vskip 0.5cm
	

	\newpage
	
	\section{Short questions (20 points)}
	\textbf{Please answer any four of the short questions.}
	\text{If you do all 5, I will do best 4 out of 5.}
	\begin{enumerate}	
			\item (5 pts) Let $X_1, X_2, \dots , X_n$ be
		i.i.d. samples of random variable with density $f$ on the real line. A standard estimate
		of $f$ is the kernel density estimate 
		\begin{align*}
		\hat{f}(x)=\frac{1}{nh}\sum_{i=1}^nK\left(\frac{x-X_i}{h}\right)
		\end{align*}
		where $K:\Re\rightarrow [0,\infty)$ is a kernel function satisfying $\int_{-\infty}^\infty K(t)dt=1$, and $h$ is a bandwidth parameter. We will measure the quality of $\hat{f}$ using
		$\|\hat{f}-f\|_1:=\int_{-\infty}^\infty |\hat{f}(t)-f(t)|dt.$
		Prove that:
		$$	P(\|\hat{f}-f\|_1\geq E\|\hat{f}-f\|_1+\delta)\leq e^{-cn\delta^2},$$
		where $c$ is some constant.
	\item (5 pts) 
Let $X_1,\dots,X_n$ be IID $\text{uniform}(0,\tau)$ random variables. Consider the U statistic $U_n$ corresponding to the kernel $h(x,y)=|x-y|$. What is the limiting distribution of this U statistic? Write your answer in the following form:
$$a_n(U_n-\theta_1)\cd X,\qquad X\sim f_{\theta_2}$$
where $a_n$ is some deterministic sequence, $\theta_1=E U_n$ and $f$ is the limiting distribution (e.g. normal/exponential etc ) parametrized by $\theta_2$. Provide expressions for $a_n,\theta_1,\theta_2$ and specify what the limiting distribution is.

\item (5 pts)  Let $X_1,\dots,X_n$ be independent and suppose that $X_n=\sqrt{n}$ with probability $1/2$ and $-\sqrt{n}$ with probability $1/2$, for $n=1,2,\dots$. Find the asymptotic distribution of $\bar{X}_n$. 

\item (5 pts) Consider datapoints in a two dimensional unit circle centered at the origin. Now consider a function class $\mathcal{F}_x$ which is a set of linear classifiers in $\mathbb{R}^2$ such that the distance of any datapoint from the classifier is at least $x$, where $x$ is some non-negative number. What is the VC-dimension of $\mathcal{F}_{3/4}$? 
	


	\item (5 pts) Consider the autoregressive sequence:
	$$X_n=\beta X_{n-1}+\epsilon_n,\ \ \mbox{for } n=1,2,\dots$$
	where $\epsilon_1$, $\epsilon_2,\dots$ are IID random variables with $E[\epsilon_i]=\mu$ and $\var(\epsilon_i)=\sigma^2$. $X_0=0$ and $-1\leq \beta< 1$.
	Show that
	\begin{enumerate}
		\item For $\beta\in(-1,1)$, 
		$$\sqrt{n}\left(\bar{X}_n-\frac{\mu}{1-\beta}\right)\cd N\left(0,\frac{\sigma^2}{(1-\beta)^2}\right)$$
		\item For $\beta=-1$,
		$$\sqrt{n}\left(\bar{X}_n-\frac{\mu}{2}\right)\cd N\left(0,\frac{\sigma^2}{2}\right)$$
	\end{enumerate}
\bl



	\ib
	
	
	\newpage
	\section{Long questions (30 points)}
\textbf{Please answer any three of the long questions.} \text{If you do all 4, I will do best 3 out of 4.}
	\bi
\item (10 pts) Let $X_i\in\bR^p,i=1\dots n$ be IID random variables such that $X_i\sim N(0,I_{p\times p})$ where $I_{p\times p}$ is the $p\times p$ identity matrix. Define the function class $\F=\{f:\bR^p\rightarrow \bR| f(x_1,\dots, x_p)=\beta^T x; \|\beta\|_1\leq R\}$, where $\beta^T x=\sum_{i=1}^p\beta_ix_i$. We will prove that $\sup_{f\in\mathcal{F}} \left|\frac{1}{n}\sum_i f(X_i)-E[f(X_1)]\right|\cp 0$. 
\bi
\p (5pts) Show that $$\sup_{f\in\mathcal{F}} \left|\frac{1}{n}\sum_i f(X_i)-E[f(X_1)]\right|\leq \frac{R}{\sqrt{n}}\max_{1\leq j\leq p} |Z_j|,$$
where $Z_j$'s are IID standard normal random variables.

\p (5pts) Now show that, as long as $R\sqrt{\log p/n}\rightarrow 0$, $$\sup_{f\in\mathcal{F}} \left|\frac{1}{n}\sum_i f(X_i)-E[f(X_1)]\right|\cp 0.$$
\ib
\item (10 pts) Consider the set with $s$ sparse vectors in the Euclidean unit ball. The $\|u\|_0$ norm counts the number of non-zero elements in a vector $u$.
$$\mathcal{S}_d(s)=\{\theta\in \mathbb{R}^d|\|\theta\|_0\leq s, \|\theta\|_2\leq 1\}.$$
We will prove the following bound on the Gaussian complexity 
\begin{align}\label{eq:gauss}
\mathcal{G}(\mathcal{S}_d(s))\leq C\sqrt{s\log(ed/s)},
\end{align}
for some constant $C$.
\begin{enumerate}
	\item (Bonus points:) Show that $\mathcal{G}(\mathcal{S}_d(s))=E\left[\max_{|S|=s}\|w_S\|_2\right]$
where $w_S\in \mathbb{R}^{|S|}$ is the sub-vector of $w_1,\dots, w_d$ indexed by the subset $S\subset\{1,\dots,d\}$.
\item (4 pts) Show that for any fixed subset $S$ of size $s$, 
$$P\left(\|w_S\|_2\geq \sqrt{s}+\delta\right)\leq \exp(-\delta^2/2),$$
where $C$ is some positive constant.
\item (6 pts) Use (a) and (b) to prove Eq~\ref{eq:gauss}.
\end{enumerate}
\item (10 pts) Consider a random undirected network, where $A_{ij}=A_{ji}\stackrel{iid}{\sim} Bernoulli(p)$ for $1\leq i<j\leq n$. $A_{ii}=0$ for $1\leq i\leq n$. Let $T$ denote the number of triangles in this graph.
\bi
\p (5 pts)  Show that the variance of $T$ is $${n\choose 3}(p^3-p^6)+c_1{n\choose 4}(p^5-p^6),$$
where $c_1$ is a universal constant.
\p (5 pts) Now use the Efron Stein inequality to obtain an upper bound on the variance. Use the true variance as a guideline to get a tight upper bound. 
\ib
\item (10 pts) Let $X_1,\dots, X_n$ be IID random variables. Consider the following statistic $$Y_n=\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n h(X_{i},X_j),$$
where $h$ is a bounded symmetric kernel with $Eh^2(X_1,X_2)<\infty$, and\\ $\var(E[h(X_1,X_2)|X_1])>0$. 
\begin{enumerate}
	\item (4 pts)  Is $Y_n$ an unbiased estimator of $\theta:=Eh(X_1,X_2)$? Prove or provide a counter example.
	\item (5 pts)  Establish the asymptotic distribution of $\sqrt{n}(Y_n-\theta)$  (you need to provide the limiting distribution and the parameter of the limiting distribution, like short question 2). Clearly state what results you are using for full credit. \textit{Hint: You can use existing results on convergence of $U$ statistics. }
	\item (1 pt) Do you think we can find a symmetric kernel $g$ such that 
	$$\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n h(X_{i},X_j)=\frac{1}{{n\choose 2}}\sum_{i<j} g(X_i,X_j).$$
	Why or why not? 
\end{enumerate}

\end{enumerate}	
	

\end{document}
