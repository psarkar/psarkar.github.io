\documentclass[11pt]{article}
\usepackage[left=1.25in,top=1in,right=1.25in,bottom=1.00in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{amsbsy}
\usepackage{amsthm}
\usepackage{hyperref}

\usepackage{epsfig}
\usepackage{color}
\usepackage[round]{natbib}
\usepackage{multirow} 
\newcommand{\logit}{\mbox{logit}}
\newcommand{\probit}{\mbox{probit}}
\newcommand{\hiw}{{\small\textsc{HIW}}}
\newcommand{\iw}{{\small\textsc{IW}}}
\newcommand{\N}{\mbox{N}}
\newcommand{\Be}{\mbox{Be}}
\newcommand{\dd}{\mbox{d}}
\newcommand{\C}{\; | \;}
\newcommand{\R}{\mathbb{R}}
\newcommand{\var}{\text{var}}
%\newcommand{\R}{\mathcal{R}}
\newcommand{\F}{\mathcal{F}}

\renewcommand{\S}{\mathcal{S}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}




\begin{document}

\title{{\bf Homework Assignment 4}\\Due via Canvas, April 17th by midnight}
\author{SDS 384-11 Theoretical Statistics}

\date{}

\maketitle{}
%\textbf{Set algebra and probability laws.}
\begin{enumerate}%\item Read Bertsekas and Tsitsiklis, sections 1.1, 1.2 and 1.3.
%\section{Jeffrey's prior}
%\begin{enumerate}
%\end{enumerate}

\item Consider an i.i.d. sample of size $n$ from a discrete distribution parametrized by $p_1,\dots, p_{m-1}$  on $m$ atoms. A common test for uniformity of the distribution is to look at the fraction of pairs that collide, or are equal. Call this statistic $U$.
\begin{enumerate}
	%	\item What is the variance of $U$?
	\item Is $U$ a U statistic? When is it degenerate?
	\item What is the variance of $U$? Please give the exact answer, without approximation. 
	\item For a hypothesis test, we will consider alternative distributions which have $p_i=\frac{1+a}{m}$ for half of the atoms in the distribution and $\frac{1-a}{m}$ for the other half ($0\le a\le 1$), for some $a>0$. Assume that there are an even number of atoms. (Hint: think of this as a multinomial distribution.)%Under the alternative, what is the asymptotic distribution of the statistic $U$?
	\begin{enumerate}
		\item What are the mean and variance of this statistic under the null?
		\item What are the mean and variance of this under the alternative?
		\item What is the asymptotic distribution of $U$ under the null hypothesis that $p_i=1/m$? \textit{Hint: you can use the fact that for $X_1,\dots, X_N\stackrel{i.i.d}{\sim} multinomial(q_1,\dots,q_k)$, $\sum_{i=1}^k (N_i-Nq_i)^2/Nq_i\stackrel{d}{\rightarrow} \chi^2_{k-1}$, where $N_i$ is the number of datapoints with value $i$.}
		\item Under the alternative hypothesis,is it always the case that $U$ has a limiting normal distribution? Can you give a sufficient condition on the number of atoms $m$  so that this is true?  
		\textit{Hint: Your variance will have two parts, and when the first one (with $1/n$ dependence on $n$) dominates the second (with $1/n^2$ dependence on $n$), you have a normal convergence. Typically, if $m$ is small, the first one will dominate, however, it is possible that $m$ is very large, in so you need $n$ to be sufficiently large for the first term to dominate the second. }
		%\item Write down the probability of accepting the null hypothesis (which is $p_i=1/m$, for all $i$), when in fact the underlying distribution if coming from the non-uniform distribution parametrized by $a$ described above.
		%\item How big does $n$ and $a$ have to be so that the above probability to be smaller than some small fraction $\delta$? To be concrete, provide a lower bound on $n$ in terms of $m,\ \delta$ and $\epsilon$.
	\end{enumerate}
\end{enumerate}
%\item Look at the seminar paper ``Probability Inequalities for Sums of Bounded Random Variables'' by Wassily Hoeffding. It should be available via \url{lib.utexas.edu}. Read and reproduce the proof of equation 5.7 for large sample deviation of order $r$ U statistics.
\item (7 pts) Look at the seminal paper ``Probability Inequalities for Sums of Bounded Random Variables'' by Wassily Hoeffding. It should be available via \url{lib.utexas.edu}. You can assume that $n$ is a multiple of $m$ (the degree of the kernel). Assume that the kernel is bounded, i.e. $|h(X_1,\dots,X_m)-\theta|\leq b$, where $\theta=E[h(X_1,\dots, X_m)]$.
\begin{enumerate}
	\item Read and reproduce the proof of equation 5.7 for large sample deviation of order $m$ U statistics. 
	\item Also prove Bernstein's inequality (see below) for U statistics. This is buried in the paper, you will have to find the bits and pieces and put them together. The Bernstein inequality is given by:
	\begin{align*}
	P(|U_n-\theta|\geq \epsilon)\leq a\exp\left(-\frac{ n \epsilon^2/m}{c_1\sigma^2+c_2 \epsilon}\right),
	\end{align*}
	where $\sigma^2=\var(h(X_1,\dots,X_m))$ and $a,c_1,c_2$ are universal constants.
\end{enumerate}

  \item Compute the VC dimension of the following function classes. You can take it as everything on or inside the shape is +ve.
\begin{enumerate}
	\item Circles in $\R^2$ 
	\item Axis aligned rectangles in $\R^2$
	\item Axis aligned squares in $\R^2$
\end{enumerate}
\end{enumerate}
\end{document} 
